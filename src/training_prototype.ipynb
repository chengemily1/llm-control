{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2163c12e-c2df-45a3-9c3b-d1557c5046c2",
   "metadata": {
    "id": "2163c12e-c2df-45a3-9c3b-d1557c5046c2"
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets # Only needed to run on Google Colab\n",
    "\n",
    "import argparse\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import random\n",
    "from sklearn.metrics import f1_score \n",
    "from scipy import optimize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c95ad2-f24a-4d7e-8c6f-eddd16a7add9",
   "metadata": {
    "id": "a1c95ad2-f24a-4d7e-8c6f-eddd16a7add9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining proof-of-concept\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Data selection\u001b[39;00m\n\u001b[1;32m      4\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='training proof-of-concept')\n",
    "\n",
    "# Data selection\n",
    "parser.add_argument('--model_name', type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
    "parser.add_argument('--dataset_name', type=str, default='/home/echeng/llm-control/jigsaw-toxic-comment-classification-challenge')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "parser.add_argument('--gamma', type=float, default=0.01)\n",
    "parser.add_argument('--svm_thres', type=float, default=0.05)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "ACCESS_TOKEN='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c6ccbc-de20-4b57-8dbf-3a414e0ef26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear control wrapper class\n",
    "class LinearControlWrapper(torch.nn.Module):\n",
    "    def __init__(self, base_layer: nn.Module, linear_probe: nn.Module, name=\"\", gamma=0.01, thres=0.05):\n",
    "        \"\"\"\n",
    "        W shape: d x 2\n",
    "        \"\"\"\n",
    "        super(LinearControlWrapper, self).__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.thres = - thres # the SVM threshold we're comfortable with\n",
    "\n",
    "        # Probe-related parameters\n",
    "        self.gamma = gamma\n",
    "        self.probe = linear_probe.eval().half()\n",
    "        self.W = linear_probe.weight # linear probe\n",
    "        self.w1 = self.W[0,:].detach().cpu().numpy()\n",
    "        self.w2 = self.W[1,:].detach().cpu().numpy()\n",
    "        self.b = linear_probe.bias\n",
    "        self.w = self.w2 - self.w1 # as defined in algo w_2 - w_1\n",
    "        self.w_norm = np.linalg.norm(self.w) # python float\n",
    "        self.w2T_w = self.w2 @ self.w\n",
    "\n",
    "        # Logging\n",
    "        self.toxic_sequences = []\n",
    "        self.toxicity_log = []\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        print('attention_mask' in kwargs)\n",
    "        x_seq, x_metadata = self.base_layer(x, *args, **kwargs)\n",
    "        print(x_seq.shape) #need this shape to be the same as the altered shape\n",
    "\n",
    "        # Why is the next iteration of generation taking a shape of 2 x 1 x d??\n",
    "        # Now update the last token representation\n",
    "        last_token_idx = kwargs['position_ids'].cpu().size(1) - 1\n",
    "\n",
    "        x_seq[torch.arange(x_seq.size(0)),last_token_idx] += self.optimal_theta(\n",
    "            x_seq[torch.arange(x_seq.size(0)),last_token_idx] # get last token rep\n",
    "        )\n",
    "        print('Adjustment done')\n",
    "        print(x_seq.shape)\n",
    "\n",
    "        # Add to toxicity log\n",
    "        eval_probe = self.probe(x_seq[torch.arange(x_seq.size(0)),last_token_idx])\n",
    "        self.toxicity_log.append(nn.functional.softmax(eval_probe, dim=-1)[:,0].detach().cpu().numpy()) # this is the probscore\n",
    "\n",
    "        return x_seq, x_metadata\n",
    "\n",
    "    def optimal_theta(self, x):\n",
    "        \"\"\"Finds the optimal steering vector.\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        theta = torch.zeros(x.shape) # batch size x d\n",
    "        x = x.detach().cpu().numpy()\n",
    "\n",
    "        # Classified as toxic when (w_2 - w_1).T x < 0\n",
    "        toxic_sequences_idx = np.where(x @ self.w < self.thres)\n",
    "        self.toxic_sequences.append(toxic_sequences_idx)\n",
    "\n",
    "        if not len(toxic_sequences_idx[0]): \n",
    "            print('No toxic sequences')\n",
    "            return theta.to(self.W.device)\n",
    "        \n",
    "        print(\"Check which sequences are toxic\")\n",
    "\n",
    "        x_toxic = x[toxic_sequences_idx] # index into the toxic ones only\n",
    "        \n",
    "        root_function = lambda l: l * np.exp(l * self.w_norm**2 + x_toxic @ self.w) + l - 1/self.gamma\n",
    "        # min_function = lambda l: self.gamma * l**2 / 2 * self.w_norm**2 - l * self.w2T_w + np.log(\n",
    "            # np.exp(x_toxic @ self.w1 + l * (self.w1 @ self.w)) + \\\n",
    "                #    np.exp(x_toxic  @ self.w2 + l * (self.w2 @ self.w))\n",
    "            # )\n",
    "        \n",
    "        # Binary search for initial condition\n",
    "        x0 = np.ones(len(toxic_sequences_idx[0],)) * min(0.001, 0.25 * self.gamma) \n",
    "        while np.all(root_function(x0) > 0):\n",
    "            x0 = x0 * 0.5 # we know the positive root is between 0 and 1/gamma\n",
    "        x0 = 2 * x0\n",
    "\n",
    "        # print('exponent: ', exponent(x0))\n",
    "        lmbda = optimize.root(root_function, x0, tol=1e-6) # parameter to optimize\n",
    "        # bounds = optimize.Bounds(lb=np.zeros(x0.shape))\n",
    "        # lmbda = optimize.minimize(min_function, x0, bounds=bounds)\n",
    "        print(lmbda)\n",
    "\n",
    "        assert lmbda.success == True \n",
    "        theta[toxic_sequences_idx] = torch.Tensor(\n",
    "            np.expand_dims(lmbda.x, axis=1) @ np.expand_dims(self.w, axis=0)\n",
    "        )\n",
    "        return theta.to(self.W.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "734a0339-544d-469c-b35f-52ff6c4a6f30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "734a0339-544d-469c-b35f-52ff6c4a6f30",
    "outputId": "ac03a143-d9d3-41ea-dd7f-fd49c39fe8d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cefb7139ceb4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACCESS_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(args.model_name, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                              \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACCESS_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3165\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3166\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m\"and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name, token=ACCESS_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, \n",
    "                                             token=ACCESS_TOKEN,\n",
    "                                             load_in_8bit=True\n",
    "                                            )\n",
    "\n",
    "if 'Llama-2' in args.model_name:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" \n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538a65e3-0562-49e4-a3a9-eb8c07403a76",
   "metadata": {
    "id": "538a65e3-0562-49e4-a3a9-eb8c07403a76"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(args.dataset_name + '/test.csv').sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ee61a-73f5-4734-b153-908ec68230aa",
   "metadata": {
    "id": "a91ee61a-73f5-4734-b153-908ec68230aa",
    "outputId": "9e4b0929-4d5c-41d2-f096-5ce0e61d962d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95290</th>\n",
       "      <td>fec47a90d3ff4325</td>\n",
       "      <td>Hi \\n\\nHey brownaddictUK, welcome to wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60474</th>\n",
       "      <td>a1e22f184f04b2c7</td>\n",
       "      <td>\"I just have a problem with the whole opening ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>0f3a4d4d25070429</td>\n",
       "      <td>RfC: Should the infobox say that the glacier i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22717</th>\n",
       "      <td>3bfce01ba2a48b38</td>\n",
       "      <td>(i.e. outside France/US)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111645</th>\n",
       "      <td>554248e2e10efe24</td>\n",
       "      <td>Cultural Signicance \\n\\nThe article totally la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "95290   fec47a90d3ff4325  Hi \\n\\nHey brownaddictUK, welcome to wikipedia...   \n",
       "60474   a1e22f184f04b2c7  \"I just have a problem with the whole opening ...   \n",
       "5691    0f3a4d4d25070429  RfC: Should the infobox say that the glacier i...   \n",
       "22717   3bfce01ba2a48b38                           (i.e. outside France/US)   \n",
       "111645  554248e2e10efe24  Cultural Signicance \\n\\nThe article totally la...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "95290       0             0        0       0       0              0  \n",
       "60474       0             0        0       0       0              0  \n",
       "5691        0             0        0       0       0              0  \n",
       "22717       0             0        0       0       0              0  \n",
       "111645      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf143285-d133-4214-9ee5-ec6f5bab3ac7",
   "metadata": {
    "id": "cf143285-d133-4214-9ee5-ec6f5bab3ac7",
    "outputId": "8a1f9f81-9bfd-4cbc-b3a9-ac4023267271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95290     Hi \\n\\nHey brownaddictUK, welcome to wikipedia...\n",
       "60474     \"I just have a problem with the whole opening ...\n",
       "5691      RfC: Should the infobox say that the glacier i...\n",
       "22717                              (i.e. outside France/US)\n",
       "111645    Cultural Signicance \\n\\nThe article totally la...\n",
       "                                ...                        \n",
       "97774      \"::How about \"\"Social Contract Theory\"\"? - \\n\\n\"\n",
       "55176     \"\\n\\n \"\"I have no tribunal\"\" \\n\\nRevision on \"...\n",
       "40850                            I forgot myself at Zilina.\n",
       "26348     '''xenophobic a highly perjorative term alludi...\n",
       "54685     Track map? \\n\\nCan a track map be made by jdjo...\n",
       "Name: comment_text, Length: 1596, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894cf20-a7cc-4750-bc52-816fd7111df8",
   "metadata": {
    "id": "6894cf20-a7cc-4750-bc52-816fd7111df8"
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a74ede-6db3-4b20-9cd3-59758ea88864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load linear probe\n",
    "W = torch.load('/home/echeng/llm-control/experiments/toxicity/linear_probe_tiny.pt').to(args.device)\n",
    "W.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f78684-14c8-43cc-ad83-8e58921a5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the model layer with the control wrapper\n",
    "model.model.layers[31] = LinearControlWrapper(model.model.layers[31], W, gamma=args.gamma, thres=args.svm_thres)\n",
    "model.model.layers[31].eval()\n",
    "\n",
    "############### REPLACE WITH TEST DATA POINTS #####################\n",
    "data = ['something toxic'] \n",
    "# data = list(dataset['comment_text'])[:5] # UNCOMMENT IF YOU WANT TO WORK WITH ACTUAL DATASET\n",
    "###################################################################\n",
    "\n",
    "def encode_data(tokenizer, N, data, batch_size, max_length, device, last_k=None):\n",
    "    # If the input data is text\n",
    "    if type(data[0]) == str:\n",
    "        encodings = tokenizer(data, padding=True, truncation=True, max_length=max_length, return_length=True, return_tensors=\"pt\") # output variable length encodings\n",
    "        if not last_k:\n",
    "            encodings = [\n",
    "                {'input_ids': encodings['input_ids'][i: i + batch_size].to(device),\n",
    "                'attention_mask': encodings['attention_mask'][i: i + batch_size].to(device),\n",
    "                'length': encodings['length'][i: i + batch_size] }\n",
    "                for i in range(0, N, batch_size)\n",
    "            ]\n",
    "        else:\n",
    "            encodings = [\n",
    "                {'input_ids': encodings['input_ids'][i: i + batch_size][-last_k:].to(device),\n",
    "                'attention_mask': encodings['attention_mask'][i: i + batch_size][-last_k:].to(device) }\n",
    "                for i in range(0, N, batch_size)\n",
    "            ]\n",
    "    else: # input data is tokens-- manually pad and batch.\n",
    "        max_len = max([len(sentence) for sentence in data])\n",
    "        data = [sentence for sentence in data if len(sentence) > 2]\n",
    "        encodings = [tokenizer.encode(sentence[1:], padding='max_length', max_length=max_len, return_tensors=\"pt\") \\\n",
    "                     for sentence in data]\n",
    "        batched_encodings = [torch.stack(encodings[i: i + batch_size]).squeeze(1).to(device) for i in range(0, len(data), batch_size)]\n",
    "        batched_attention_masks = [(tokens != 1).to(device).long() for tokens in batched_encodings]\n",
    "        encodings = [\n",
    "            {'input_ids': batched_encodings[j], 'attention_mask': batched_attention_masks[j]}\n",
    "            for j in range(len(batched_encodings))\n",
    "        ]\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59b16d-b79f-4de5-9e1e-0bf7be400d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "encodings = encode_data(tokenizer, len(data), data, args.batch_size, model.config.max_position_embeddings, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b604be-5db8-4e67-a762-61608583b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "model.eval()\n",
    "outputs = model.generate(\n",
    "    inputs=torch.concat([encoding['input_ids'] for encoding in encodings], axis=0), # batch size x seq len \n",
    "    max_new_tokens=10\n",
    ")\n",
    "output_text = tokenizer.batch_decode(outputs)\n",
    "print('OUTPUT: ', output_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
